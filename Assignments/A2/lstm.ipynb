{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 style=\"text-align: center;\">INLP - Assignment 2</h1>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <p>Name: Vedant Nipane</p>\n",
    "    <p>Roll No: 2021102040</p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "text1_path = 'Pride and Prejudice - Jane Austen.txt'\n",
    "text2_path = 'Ulysses - James Joyce.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load text and split into sentences\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()  # Convert to lowercase\n",
    "\n",
    "    sentences = re.split(r'[.!?]', text)  # Split on sentence boundaries\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty lines\n",
    "    return sentences\n",
    "\n",
    "# Split into train and test (1000 test sentences, rest train)\n",
    "def split_data(sentences, test_size=1000):\n",
    "    random.shuffle(sentences)  # Shuffle sentences to ensure randomness\n",
    "    test_sentences = sentences[:test_size]\n",
    "    train_sentences = sentences[test_size:]\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "# Tokenize a list of sentences\n",
    "def tokenize(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence).split()  # Remove special chars & split\n",
    "        tokenized_sentences.append(words)\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Build vocabulary from training data only\n",
    "def build_vocab(tokenized_sentences, min_freq=2):\n",
    "    word_counts = Counter(word for sentence in tokenized_sentences for word in sentence)\n",
    "    vocab = {word: idx for idx, (word, freq) in enumerate(word_counts.items()) if freq >= min_freq}\n",
    "    vocab[\"<UNK>\"] = len(vocab)  # Add unknown token\n",
    "    return vocab\n",
    "\n",
    "# Convert words to numerical indices\n",
    "def words_to_indices(sentences, vocab):\n",
    "    indexed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        indexed_sentences.append([vocab.get(word, vocab[\"<UNK>\"]) for word in sentence])\n",
    "    return indexed_sentences\n",
    "\n",
    "# Generate n-gram dataset\n",
    "def create_ngrams(indexed_sentences, n=3):\n",
    "    data = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) >= n:\n",
    "            for i in range(len(sentence) - n):\n",
    "                context = sentence[i : i + n]  # First (n) words\n",
    "                target = sentence[i + n]  # Next word (prediction target)\n",
    "                data.append((context, target))\n",
    "    return data\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "def prepare_tensors(data):\n",
    "    contexts = torch.tensor([x[0] for x in data], dtype=torch.long)\n",
    "    targets = torch.tensor([x[1] for x in data], dtype=torch.long)\n",
    "    return contexts, targets\n",
    "\n",
    "\n",
    "# Function to pad sequences\n",
    "def pad_sentences(indexed_sentences, pad_token=0, max_len=None):\n",
    "    \"\"\"Pad sentences to the same length for batch processing.\"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in indexed_sentences)  # Find longest sequence\n",
    "\n",
    "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in indexed_sentences]\n",
    "    return padded_sentences\n",
    "\n",
    "# Function to prepare LSTM tensors\n",
    "def prepare_lstm_tensors(indexed_sentences, vocab, pad_token=0):\n",
    "    \"\"\"Convert sentences to tensors with padding.\"\"\"\n",
    "    indexed_sentences = pad_sentences(indexed_sentences, pad_token)\n",
    "    sequences = torch.tensor(indexed_sentences, dtype=torch.long)\n",
    "    targets = torch.tensor([s[1:] + [pad_token] for s in indexed_sentences], dtype=torch.long)  # Shifted targets\n",
    "\n",
    "    return sequences, targets  # (Batch_size, Seq_len), (Batch_size, Seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Corpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (1): 6620\n",
      "Train LSTM Dataset Size (1): torch.Size([6519, 118])\n",
      "Test LSTM Dataset Size (1): torch.Size([1000, 112])\n"
     ]
    }
   ],
   "source": [
    "# Load text and preprocess\n",
    "sentences1 = load_text(text1_path)\n",
    "train_sentences1, test_sentences1 = split_data(sentences1)\n",
    "\n",
    "train_tokens1 = tokenize(train_sentences1)\n",
    "test_tokens1 = tokenize(test_sentences1)\n",
    "\n",
    "train_vocab1 = build_vocab(train_tokens1, min_freq=1)\n",
    "# Create a mapping from indices to words (inverse of train_vocab)# Convert words to indices\n",
    "train_indices1 = words_to_indices(train_tokens1, train_vocab1)\n",
    "test_indices1 = words_to_indices(test_tokens1, train_vocab1)  # Use same vocab\n",
    "\n",
    "# Convert to tensors for LSTM\n",
    "X_train_lstm1, y_train_lstm1 = prepare_lstm_tensors(train_indices1, vocab=train_vocab1)\n",
    "X_test_lstm1, y_test_lstm1 = prepare_lstm_tensors(test_indices1, vocab=train_vocab1)\n",
    "\n",
    "idx_to_word1 = {idx: word for word, idx in train_vocab1.items()}\n",
    "\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Vocabulary Size (1): {len(train_vocab1)}\")\n",
    "print(f\"Train LSTM Dataset Size (1): {X_train_lstm1.shape}\")\n",
    "print(f\"Test LSTM Dataset Size (1): {X_test_lstm1.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Corpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (2): 29023\n",
      "Train LSTM Dataset Size (2): torch.Size([24427, 12824])\n",
      "Test LSTM Dataset Size (2): torch.Size([1000, 155])\n"
     ]
    }
   ],
   "source": [
    "# Load text and preprocess\n",
    "sentences2 = load_text(text2_path)\n",
    "train_sentences2, test_sentences2 = split_data(sentences2)\n",
    "\n",
    "train_tokens2 = tokenize(train_sentences2)\n",
    "test_tokens2 = tokenize(test_sentences2)\n",
    "\n",
    "train_vocab2 = build_vocab(train_tokens2, min_freq=1)\n",
    "train_indices2 = words_to_indices(train_tokens2, train_vocab2)\n",
    "test_indices2 = words_to_indices(test_tokens2, train_vocab2)\n",
    "\n",
    "# Convert to tensors for LSTM\n",
    "X_train_lstm2, y_train_lstm2 = prepare_lstm_tensors(train_indices2, vocab=train_vocab2)\n",
    "X_test_lstm2, y_test_lstm2 = prepare_lstm_tensors(test_indices2, vocab=train_vocab2)\n",
    "\n",
    "idx_to_word2 = {idx: word for word, idx in train_vocab2.items()}\n",
    "\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Vocabulary Size (2): {len(train_vocab2)}\")\n",
    "print(f\"Train LSTM Dataset Size (2): {X_train_lstm2.shape}\")\n",
    "print(f\"Test LSTM Dataset Size (2): {X_test_lstm2.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.3):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)  # Convert indices to embeddings\n",
    "        x, hidden = self.lstm(x, hidden)  # Pass through LSTM\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc(x)  # Project to vocabulary size\n",
    "        return x, hidden  # (Batch, Seq_len, Vocab_size), hidden states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Perplexity Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_perplexity_lstm(model, sentence_indices, device):\n",
    "    \"\"\"Computes perplexity for a single sentence using an LSTM model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if not sentence_indices or len(sentence_indices) < 2:  # Skip invalid sentences\n",
    "        return None  \n",
    "\n",
    "    # Convert sentence indices to tensor and move to device\n",
    "    sentence_tensor = torch.tensor(sentence_indices, dtype=torch.long, device=device).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    \n",
    "    batch_size = 1  # Since we process one sentence at a time\n",
    "    num_layers = model.lstm.num_layers  # Get LSTM num_layers\n",
    "    hidden_dim = model.lstm.hidden_size  # Get LSTM hidden_size\n",
    "\n",
    "    hidden = torch.zeros(num_layers, batch_size, hidden_dim, device=device)\n",
    "    cell = torch.zeros(num_layers, batch_size, hidden_dim, device=device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sentence_indices) - 1):  # Predict each word given previous ones\n",
    "            input_word = sentence_tensor[:, i].unsqueeze(1)  # Shape: (1, 1)\n",
    "            target_word = sentence_tensor[:, i + 1]  # Shape: (1,)\n",
    "\n",
    "            output, (hidden, cell) = model(input_word, (hidden, cell))  # Forward pass\n",
    "            output = output.squeeze(1)  # Remove sequence dimension (1, vocab_size)\n",
    "\n",
    "            loss = loss_function(output, target_word)  # Compute loss\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    avg_loss = total_loss / count if count > 0 else float('inf')\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()  # Convert loss to perplexity\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def save_perplexity_results_lstm(corpus_name, dataset_type, sentences_indices, perplexities):\n",
    "    \"\"\"Save perplexity results to file for LSTM\"\"\"\n",
    "    file_name = f\"2021102040_lstm_{corpus_name}_{dataset_type}-perplexity.txt\"\n",
    "    file_path = os.path.join('Perplexity', file_name)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        avg_perplexity = np.mean([p for p in perplexities if p != float('inf')])\n",
    "        f.write(f\"Overall Average Perplexity: {avg_perplexity:.2f}\\n\\n\")\n",
    "        \n",
    "        for idx, (sentence, perp) in enumerate(zip(sentences_indices, perplexities), 1):\n",
    "            f.write(f\" {sentence} - Perplexity: {perp:.2f}\\n\")\n",
    "\n",
    "def evaluate_and_save_perplexity_lstm(model, sentences_indices, corpus_name, dataset_type, device='cpu'):\n",
    "    \"\"\"Evaluate perplexity for each sentence using an LSTM and save results\"\"\"\n",
    "    perplexities = []\n",
    "\n",
    "    for sentence in sentences_indices:\n",
    "        if len(sentence) > 1:  # Ensure valid sentence length\n",
    "            perp = calculate_sentence_perplexity_lstm(model, sentence, device)\n",
    "            if perp != float('inf'):\n",
    "                perplexities.append(perp)\n",
    "\n",
    "    save_perplexity_results_lstm(corpus_name, dataset_type, sentences_indices, perplexities)\n",
    "    return np.mean(perplexities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_helper(model,train_vocab,X_train,y_train,num_epochs,batch_size):\n",
    "    vocab_size = len(train_vocab)\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    batch_size = 32  # Define a batch size\n",
    "    epoch_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i : i + batch_size]\n",
    "            y_batch = y_train_tensor[i : i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(X_batch)\n",
    "\n",
    "            # Reshape outputs for loss computation\n",
    "            loss = criterion(output.view(-1, vocab_size), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (len(X_train_tensor) / batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        epoch_loss.append(avg_loss)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Corpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLanguageModel(\n",
       "  (embedding): Embedding(6605, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=6605, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(train_vocab1)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# Instantiate model\n",
    "model_lstm_1 = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model_lstm_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedan\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\vedan\\AppData\\Local\\Temp\\ipykernel_4548\\657519551.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
      "C:\\Users\\vedan\\AppData\\Local\\Temp\\ipykernel_4548\\657519551.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 6.7679\n",
      "Epoch 2/5 - Loss: 6.3088\n",
      "Epoch 3/5 - Loss: 5.9350\n",
      "Epoch 4/5 - Loss: 5.6547\n",
      "Epoch 5/5 - Loss: 5.4485\n"
     ]
    }
   ],
   "source": [
    "loss_epoch1 = train_helper(model_lstm_1,train_vocab1,X_train_lstm1,y_train_lstm1,num_epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM Model and metadata saved at: Models/2021102040_lstm_corpus1.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Models/2021102040_lstm_corpus1.pth\"\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "save_data = {\n",
    "    \"model_state_dict\": model_lstm_1.state_dict(),  # Model weights\n",
    "    \"word_to_idx\": train_vocab1,  # Vocabulary mapping\n",
    "    \"idx_to_word\": idx_to_word1,  # Reverse mapping\n",
    "    \"hyperparams\": {\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"vocab_size\": len(train_vocab1),\n",
    "        \"num_layers\": num_layers,  # Number of LSTM layers\n",
    "    },\n",
    "}\n",
    "torch.save(save_data, model_path)\n",
    "print(f\"✅ LSTM Model and metadata saved at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Corpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLanguageModel(\n",
       "  (embedding): Embedding(28998, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=28998, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(train_vocab2)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# Instantiate model\n",
    "model_lstm_2 = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model_lstm_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedan\\AppData\\Local\\Temp\\ipykernel_10280\\657519551.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "loss_epoch2 = train_helper(model_lstm_2,train_vocab2,X_train_lstm2,y_train_lstm2,num_epochs=5,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved at: Models/2021102040_lstm_corpus1.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"Models/2021102040_lstm_corpus2.pt\"\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "torch.save(model_lstm_2.state_dict(), model_path)\n",
    "print(f\"✅ Model saved at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Perplexity Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for Corpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 7501.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute and save train perplexity\n",
    "train_perplexity1 = evaluate_and_save_perplexity_lstm(\n",
    "    model_lstm_1, train_indices1, corpus_name=\"corpus1\", dataset_type=\"train\", device=device\n",
    ")\n",
    "print(f\"Train Perplexity: {train_perplexity1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 24496.74\n"
     ]
    }
   ],
   "source": [
    "# Compute and save test perplexity\n",
    "test_perplexity1 = evaluate_and_save_perplexity_lstm(\n",
    "    model_lstm_1, test_indices1, corpus_name=\"corpus1\", dataset_type=\"test\", device=device\n",
    ")\n",
    "print(f\"Test Perplexity: {test_perplexity1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for Corpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 7501.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute and save train perplexity\n",
    "train_perplexity2 = evaluate_and_save_perplexity_lstm(\n",
    "    model_lstm_2, train_indices2, corpus_name=\"corpus2\", dataset_type=\"train\", device=device\n",
    ")\n",
    "print(f\"Train Perplexity: {train_perplexity2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 24496.74\n"
     ]
    }
   ],
   "source": [
    "# Compute and save test perplexity\n",
    "test_perplexity2 = evaluate_and_save_perplexity_lstm(\n",
    "    model_lstm_2, test_indices2, corpus_name=\"corpus2\", dataset_type=\"test\", device=device\n",
    ")\n",
    "print(f\"Test Perplexity: {test_perplexity2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_top_k_words_lstm(model, context_sentence, vocab, k, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict the top-k next words for a given context sentence using the LSTM model.\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained LSTM model.\n",
    "    - context_sentence: List of word indices representing the input sentence.\n",
    "    - vocab: Dictionary mapping indices to words.\n",
    "    - k: Number of top words to return.\n",
    "    - device: 'cpu' or 'cuda'.\n",
    "    \n",
    "    Returns:\n",
    "    - List of (word, probability) tuples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert context sentence to tensor (batch_size=1, seq_len)\n",
    "    context_tensor = torch.tensor(context_sentence, dtype=torch.long, device=device).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "    # Retrieve LSTM parameters\n",
    "    num_layers = model.lstm.num_layers\n",
    "    hidden_dim = model.lstm.hidden_size\n",
    "\n",
    "    # Initialize hidden and cell states\n",
    "    hidden = torch.zeros(num_layers, 1, hidden_dim, device=device)  # Shape: (num_layers, batch=1, hidden_dim)\n",
    "    cell = torch.zeros(num_layers, 1, hidden_dim, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Ensure input is correctly shaped for the model\n",
    "        context_tensor = context_tensor.view(1, -1)  # Reshape to (batch=1, seq_len)\n",
    "        \n",
    "        # Pass through embedding layer manually if needed\n",
    "        if hasattr(model, \"embedding\"):  \n",
    "            context_tensor = model.embedding(context_tensor)  # Shape: (1, seq_len, embedding_dim)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        output, _ = model.lstm(context_tensor, (hidden, cell))  \n",
    "\n",
    "    # Get last word's output probabilities\n",
    "    last_word_logits = output[:, -1, :]  # (1, vocab_size)\n",
    "    probabilities = F.softmax(last_word_logits, dim=-1)  # Apply softmax to get probabilities\n",
    "\n",
    "    # Get top-k words\n",
    "    top_k_probs, top_k_indices = torch.topk(probabilities, k)  # (1, k)\n",
    "\n",
    "    # Convert indices to words\n",
    "    top_k_words = [(vocab[idx.item()], top_k_probs[0, i].item()) for i, idx in enumerate(top_k_indices[0])]\n",
    "\n",
    "    return top_k_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_to_tensor(sentence, word_to_idx, context_size, device='cpu'):\n",
    "    \"\"\"\n",
    "    Convert a sentence into a tensor format suitable for the RNN model.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        word_to_idx (dict): Mapping from words to indices.\n",
    "        context_size (int): Number of words used as context.\n",
    "        device (str): Device to place the tensor ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Context tensor of shape (1, context_size).\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence).split()\n",
    "    \n",
    "    if len(words) < context_size:\n",
    "        raise ValueError(f\"Input sentence must have at least {context_size} words\")\n",
    "    \n",
    "    # Extract the last `context_size` words\n",
    "    context_words = words[-context_size:]\n",
    "    \n",
    "    # Convert words to indices\n",
    "    context_indices = [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in context_words]\n",
    "    \n",
    "    # Convert to tensor (1, context_size)\n",
    "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    return context_tensor\n",
    "\n",
    "\n",
    "def print_predictions(sentence, predictions):\n",
    "    print(f\"\\nInput sentence: {sentence}\")\n",
    "    print(\"Top 5 predicted next words:\")\n",
    "    for word, prob in predictions:\n",
    "        print(f\"{word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 0.009940087795257568), ('intervals', 0.009896052069962025), ('elizabeth', 0.009778119623661041)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_sentence = \"I hate to\"\n",
    "context_tensor = sentence_to_tensor(context_sentence, train_vocab1, context_size=3, device='cpu')\n",
    "\n",
    "# Fix: Convert tensor back to list of indices\n",
    "top_words = predict_top_k_words_lstm(model_lstm_1, context_tensor.tolist()[0], idx_to_word1, k=3, device='cpu')\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLanguageModel(\n",
       "  (embedding): Embedding(6620, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=6620, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved checkpoint\n",
    "model_path = \"Models/2021102040_lstm_corpus1.pth\"\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "# Restore vocabulary and prevent dynamic resizing\n",
    "word_to_idx = checkpoint[\"word_to_idx\"]\n",
    "idx_to_word = checkpoint[\"idx_to_word\"]\n",
    "\n",
    "# Use stored vocab size\n",
    "vocab_size = checkpoint[\"hyperparams\"][\"vocab_size\"]\n",
    "\n",
    "# Extract hyperparameters\n",
    "embedding_dim = checkpoint[\"hyperparams\"][\"embedding_dim\"]\n",
    "hidden_dim = checkpoint[\"hyperparams\"][\"hidden_dim\"]\n",
    "num_layers = checkpoint[\"hyperparams\"][\"num_layers\"]\n",
    "\n",
    "# Build model with correct vocab size\n",
    "model_lstm_loaded = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Load model weights\n",
    "model_lstm_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model_lstm_loaded.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sentence: I know that\n",
      "Top 5 predicted next words:\n",
      "intervals: 0.0095\n",
      "little: 0.0093\n",
      "f: 0.0092\n",
      "declared: 0.0091\n",
      "play: 0.0088\n"
     ]
    }
   ],
   "source": [
    "context_sentence = \"I know that\"\n",
    "context_tensor = sentence_to_tensor(context_sentence, word_to_idx=word_to_idx, context_size=3, device='cpu')\n",
    "\n",
    "# Fix: Convert tensor back to list of indices\n",
    "top_words = predict_top_k_words_lstm(model_lstm_loaded, context_tensor.tolist()[0], idx_to_word, k=5, device='cpu')\n",
    "print_predictions(context_sentence,top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
